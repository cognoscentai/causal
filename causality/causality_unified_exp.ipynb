{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import datetime\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import precision_recall_fscore_support as prfs\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, auc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data as data_utils\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..', '..'))\n",
    "from causal.settings.filepath import cau_PATH\n",
    "import causal.estimators.entropy_estimators as ee\n",
    "from causal.estimators.Gaussian_estimators import get_entropy_Gaussian, get_entropy_Gaussian_list, get_MI_Gaussian, get_noise_entropy\n",
    "from causal.causality.models import MLP_noise, Variational_Entropy\n",
    "from causal.causality.util_causality import get_AUCs, plot_clusters, plot_examples, get_ROC_AUC\n",
    "from causal.causality.util_causality import get_PR_AUC, partition, get_shapes, get_MIs\n",
    "from causal.datasets.prepare_dataset import process_time_series, get_synthetic_data\n",
    "from causal.pytorch_net.net import MLP, Multi_MLP, LSTM, Model_Ensemble, Model_with_uncertainty, Mixture_Gaussian, train, load_model_dict\n",
    "from causal.pytorch_net.util import plot_matrices, to_Variable, to_np_array, get_activation, Loss_with_uncertainty, get_optimizer, get_criterion, shrink_tensor, expand_tensor, permute_dim, flatten, fill_triangular, matrix_diag_transform, to_string, filter_filename, sort_two_lists\n",
    "from causal.pytorch_net.logger import Logger\n",
    "from causal.util import train_test_split, Early_Stopping, record_data, make_dir, norm, format_list, get_args\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if is_cuda else \"cpu\")\n",
    "isplot = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalize = 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Data dimensions:\n",
    "X_train, X_test has dimension of (#examples, K, N), where K is the maximum time horizons, (N // group_sizes) is the number of time series,\n",
    "                                                    where group_sizes is the dimension for each time series.\n",
    "y_train, y_test has dimension of (#examples, K2, N), where K2 is the number of time steps after the input X\n",
    "\"\"\"\n",
    "############################################################\n",
    "# Choose one dataset:\n",
    "############################################################\n",
    "data_type = (\"synthetic\",\"softplus\",\"tanh\", 20, \"lognormal\", 1)    # Synthetic dataset (Section 4.1)\n",
    "# data_type = (\"breakout-CNN\",)      # breakout dataset (Section 4.2)\n",
    "# data_type = (\"sleep-apnea\",)         # heart rate vs. breath rate dataset (Section 4.3)\n",
    "# data_type = (\"ratEEG\", \"A\")        # rat brain EEG dataset, normal (Section 4.3)\n",
    "# data_type = (\"ratEEG\", \"B\")        # rat brain EEG dataset, after lesion (Section 4.3)\n",
    "\n",
    "############################################################\n",
    "# Choose one method:\n",
    "############################################################\n",
    "# method = (\"MI\", 5)\n",
    "# method = (\"trans-entropy\", 3)\n",
    "# method = (\"G-linear\", True)  # whether to fit intercept\n",
    "# method = (\"elasticNet\",)\n",
    "# method = (\"causal-influence\",0)\n",
    "method = (\"MPIR\", \"uniform-series\", \"Gaussian\", 0.002, \"info\", \"diag\")\n",
    "\n",
    "############################################################\n",
    "# Settings:\n",
    "############################################################\n",
    "# Settings if using the Jupyter notebook to run:\n",
    "exp_id = \"exp1.0\"\n",
    "N = 10                        # Number of time-series variables\n",
    "K = 3                         # Maximum time horizons for the input X\n",
    "K2 = 1                        # number of time steps for y\n",
    "id = 1                        # Legacy setting. Ineffective\n",
    "velocity_type = \"pos\"         # if \"pos\", only use the time series itself. If \"velocity\", use the difference of time series. If \"both\", concatenate the above\n",
    "normalize = 0                 # normalization type. 0 for no normalization, 2 for normalize to mean 0 and std of 1\n",
    "split_mode = (\"whole\",)       # Legacy setting. Ineffective\n",
    "reg_amp = 1e-4                # L1 regularization amplitude\n",
    "noise_type = \"uniform-series\" # If \"uniform-series\", the relative noise amplitude parameter is shared across the same time series. If \"fully-Gaussian\", \n",
    "                              # different dimension and lag of the same time series have different noise amplitude parameters.\n",
    "noise_mode = \"additive\"       # additive noise\n",
    "lr = 1e-4                     # learning rate\n",
    "group_sizes = 1               # group size for the data\n",
    "loss_core = \"mse\"             # Legacy setting. Ineffective\n",
    "num_examples = 10000          # number of examples\n",
    "batch_size = 1000             # Batch size\n",
    "struct_tuple = (8,8)          # Number of neurons for each hidden layer\n",
    "activation = \"leakyRelu\"      # Activation for hidden layers\n",
    "assigned_target_id = None     # If None, calculate causal matrix for all targets; if an integer, only calculate specific target\n",
    "date_time = \"{0}-{1}\".format(datetime.datetime.now().month, datetime.datetime.now().day)\n",
    "seed = 0                      # Seed\n",
    "idx = \"0\"                     # id of experiment\n",
    "\n",
    "# Settings if using the terminal to run:\n",
    "exp_id = get_args(exp_id, 1)\n",
    "data_type = get_args(data_type, 2, \"tuple\")\n",
    "method = get_args(method, 3, \"tuple\")\n",
    "N = get_args(N, 4, \"int\")\n",
    "true_id = id = get_args(id, 5, \"int\")\n",
    "K = get_args(K, 6, \"int\") \n",
    "K2 = get_args(K2, 7, \"int\")\n",
    "velocity_type = get_args(velocity_type, 8)\n",
    "normalize = get_args(normalize, 9, \"int\")\n",
    "split_mode = get_args(split_mode, 10, \"tuple\")\n",
    "reg_amp = get_args(reg_amp, 11, \"float\")\n",
    "lr = get_args(lr, 12, \"float\")\n",
    "loss_core = get_args(loss_core, 13)\n",
    "num_examples = get_args(num_examples, 14, \"int\")\n",
    "struct_tuple = get_args(struct_tuple, 15, \"tuple\")\n",
    "activation = get_args(activation, 16)\n",
    "assigned_target_id = get_args(assigned_target_id, 17, \"eval\")\n",
    "date_time = get_args(date_time, 18)\n",
    "seed = get_args(seed, 19, \"int\")\n",
    "idx = get_args(idx, 20)\n",
    "\n",
    "if method[0] == \"MPIR\":\n",
    "    significance_test_mode = (\"permute\", \"ratio\", 0.5)   # Significance test setting.\n",
    "else:\n",
    "    significance_test_mode = None\n",
    "\n",
    "# Seed:\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# For different methods use appropriate normalization of data:\n",
    "if method[0] in [\"elasticNet\", \"MI\", \"trans-entropy\", \"G-linear\"]:\n",
    "    normalize = 2\n",
    "elif method[0] in [\"MPIR\", \"causal-influence\"]:\n",
    "    normalize = 0\n",
    "else:\n",
    "    raise\n",
    "if data_type[0] in [\"breakout-CNN\", \"ratEEG\", \"sleep-apnea\"]:\n",
    "    normalize = 2\n",
    "print(\"normalize = {0}\".format(normalize))\n",
    "neglect_idx = \"diagonal\"  # If \"diagonal\", neglect diagonal element when calculating AUC-ROC and AUC-PR. If None, include diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory ../data/exp1.0_1-3 does not exist, created.\n",
      "../data/exp1.0_1-3/synthetic-softplus-tanh-20-lognormal-1_method_MPIR-uniform-series-Gaussian-0.002-info-diag_N_10_id_1_K_3_K2_1_vel_pos_nor_0_split_whole_reg_0.0001_lr_0.0001_core_mse_num_10000_struct_8-8_act_leakyRelu_idas_None_seed_0_idx_0.p\n"
     ]
    }
   ],
   "source": [
    "test_size = 0.1\n",
    "if data_type[0] == \"synthetic\":\n",
    "    (X, y), A_whole, B_whole, time_series = get_synthetic_data(N = N, K = K, K2 = K2, p_N = 0.5, p_K = 0.5, time_length = data_type[3], dist_type = data_type[4], mode = data_type[1], \n",
    "                                                               indi_activation = data_type[2], num_examples = num_examples, noise_variance = data_type[5], velocity_type = velocity_type, normalize = normalize, is_cuda = is_cuda, isplot = isplot)\n",
    "\n",
    "elif data_type[0] == \"sleep-apnea\":\n",
    "    # Data from https://physionet.org/physiobank/database/santa-fe/\n",
    "    # The three columns are heart rate, chest volume (respiration force) and blood oxygen concentration \n",
    "    file = \"../datasets/sleep_apnea/b1.txt\"\n",
    "    time_series = []\n",
    "    data_range = (2350, 3550)\n",
    "    with open(file, \"r\") as f:\n",
    "        for i, line in enumerate(f.readlines()):\n",
    "            if data_range[0] < i < data_range[1]:\n",
    "                time_series.append([float(element) for element in line.split(\" \")[:3]])\n",
    "        time_series = to_Variable(np.array(time_series), is_cuda = is_cuda).unsqueeze(0)        \n",
    "    time_series = time_series[...,:2]\n",
    "    time_series_plot = (time_series - time_series.mean(1, keepdim = True)) / time_series.std(1, keepdim = True)\n",
    "    X, y = process_time_series(time_series, K = K, K2 = K2, velocity_type = velocity_type, normalize = normalize, is_cuda = is_cuda)\n",
    "    if isplot:\n",
    "        plt.figure(figsize = (20, 8))\n",
    "        plt.plot(to_np_array(time_series_plot[0]))\n",
    "        plt.legend(list(range(time_series_plot.size(-1))))\n",
    "elif data_type[0] == \"ratEEG\":\n",
    "    file = \"../datasets/ratEEG/{0}-{1}.txt\".format(data_type[0], data_type[1])\n",
    "    time_series = []\n",
    "    with open(file, \"r\") as f:\n",
    "        for i, line in enumerate(f.readlines()):\n",
    "            new_line = []\n",
    "            for element in line.split(\" \"):\n",
    "                if element != \"\":\n",
    "                    if \"\\n\" in element:\n",
    "                        element = element[:-1]\n",
    "                    new_line.append(eval(element))\n",
    "            assert len(new_line) == 2\n",
    "            time_series.append(new_line)\n",
    "    time_series = to_Variable(np.array(time_series), is_cuda = is_cuda).unsqueeze(0)\n",
    "    time_series_plot = time_series\n",
    "    if data_type[1] == \"A\":\n",
    "        time_series_plot[:,:,1] = time_series_plot[:,:,1] - 1\n",
    "    else:\n",
    "        time_series_plot[:,:,1] = time_series_plot[:,:,1] - 5  # 1: Left,  0: Right\n",
    "    X, y = process_time_series(time_series, K = K, K2 = K2, velocity_type = velocity_type, normalize = normalize, is_cuda = is_cuda)\n",
    "    if isplot:\n",
    "        plt.figure(figsize = (20, 8))\n",
    "        plt.plot(to_np_array(time_series_plot[0]))\n",
    "        plt.legend(list(range(time_series_plot.size(-1))))\n",
    "elif data_type[0][:8] == \"breakout\":\n",
    "    time_series = to_Variable(pickle.load(open(\"../datasets/breakout/{0}.p\".format(data_type[0]), \"rb\")), is_cuda = is_cuda)\n",
    "    time_series = time_series[:int(num_examples / time_series.size(1))]\n",
    "    X, y = process_time_series(time_series, K = K, K2 = K2, velocity_type = velocity_type, normalize = normalize, is_cuda = is_cuda)\n",
    "else:\n",
    "    raise Exception(\"data_type {0} not recognized!\".format(data_type))\n",
    "A_whole = A_whole if \"A_whole\" in locals() else None\n",
    "N = X.size(-1)\n",
    "if velocity_type == \"both\":\n",
    "    group_sizes *= 2\n",
    "    \n",
    "# Append randomly permuted or constructed time series for significance test:\n",
    "sig_dict = {}\n",
    "if significance_test_mode is not None:\n",
    "    sig_mode = significance_test_mode[0]\n",
    "    if sig_mode == \"permute\":\n",
    "        def append_sig_to_tensor(X, sig_idx, group_sizes):\n",
    "            sig_X = X.view(-1, *X.shape[-2:])\n",
    "            sig_X_perm = []\n",
    "            for i in sig_idx:\n",
    "                sig_X_perm.append(sig_X[torch.randperm(len(sig_X)), : , i * group_sizes: (i+1) * group_sizes])\n",
    "            sig_X_perm = torch.cat(sig_X_perm, -1).view(*X.shape[:3], sig_num * group_sizes)\n",
    "            X = torch.cat([X, sig_X_perm], -1)\n",
    "            return X\n",
    "        sig_mode_1 = significance_test_mode[1]\n",
    "        if sig_mode_1 == \"ratio\":\n",
    "            sig_ratio = significance_test_mode[2]\n",
    "            sig_num = int(np.ceil(N * sig_ratio/ group_sizes))\n",
    "            sig_num_y = int(np.ceil(y.shape[-1] * sig_ratio/ group_sizes))\n",
    "        else:\n",
    "            raise\n",
    "        sig_idx = np.random.choice(int(N / group_sizes), sig_num, replace = False)\n",
    "        X = append_sig_to_tensor(X, sig_idx, group_sizes)\n",
    "        # Append y_append which is constructed to be caused by the first sig_num elements.\n",
    "        sig_y_append = []\n",
    "        control_matrix = ((1 + 0.1 * torch.randn(sig_num, K, K2)) / 3).to(device)\n",
    "        for i in range(sig_num_y):\n",
    "            sig_y_ele = torch.matmul(X[..., i * group_sizes: (i + 1) * group_sizes].transpose(-1, -2), control_matrix[i])\n",
    "            sig_y_append.append(sig_y_ele)\n",
    "        sig_y_append = torch.cat(sig_y_append, -2).transpose(-1, -2)\n",
    "        y = torch.cat([y, sig_y_append], -1)\n",
    "        \n",
    "        # Record:\n",
    "        sig_dict[\"sig_idx\"] = sig_idx\n",
    "        sig_dict[\"control_matrix\"] = control_matrix\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "if split_mode[0] == \"whole\":\n",
    "    (X_train, y_train), (X_test, y_test) = train_test_split(X, y, test_size = test_size)\n",
    "elif split_mode[0] == \"windowing\":\n",
    "    window_size = split_mode[1]\n",
    "    (X_list, y_list), info_index = slide_windows(X, y, window_size = window_size, isplot = False)\n",
    "    if False and isplot and velocity_type == \"pos\" and group_sizes == 2:\n",
    "        show_gym_video(X_list, radius = 40)\n",
    "else:\n",
    "    raise\n",
    "\n",
    "\n",
    "dirname = cau_PATH + \"/{0}_{1}/\".format(exp_id, date_time)\n",
    "filename = dirname + \"{0}_method_{1}_N_{2}_id_{3}_K_{4}_K2_{5}_vel_{6}_nor_{7}_split_{8}_reg_{9}_lr_{10}_core_{11}_num_{12}_struct_{13}_act_{14}_idas_{15}_seed_{16}_idx_{17}.p\".format(\n",
    "                        format_list(data_type, \"-\"), format_list(method, \"-\"), \n",
    "                        N, true_id, K, K2, velocity_type, normalize, format_list(split_mode, \"-\"), \n",
    "                        reg_amp, lr, loss_core, num_examples, format_list(struct_tuple, \"-\"), activation, assigned_target_id, seed, idx)\n",
    "make_dir(filename)\n",
    "print(filename)\n",
    "window_idx = range(len(X_list)) if \"X_list\" in locals() else range(100)\n",
    "causality_truth = np.abs(A_whole) > 0 if A_whole is not None else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mutual_information(X, y, group_sizes = 1, neighbors = 3, assigned_target_id = None, isplot = False):\n",
    "    \"\"\"Obtain pairwise mutual information\"\"\"\n",
    "    N, Ny, K, K2, num_models, num_models_y, group_sizes_y, training_target_ids = \\\n",
    "        get_shapes(X, y, group_sizes, assigned_target_id)\n",
    "    MI_matrix = np.zeros((num_models_y, num_models))\n",
    "\n",
    "    for i in training_target_ids:\n",
    "        for j in range(num_models):\n",
    "            if isplot:\n",
    "                print(\"target i: {0}\\tsource j: {1}\".format(i, j))\n",
    "            y_chosen = y[:, :, i * group_sizes_y: (i + 1) * group_sizes_y]\n",
    "            X_chosen = X[:, :, j * group_sizes: (j + 1) * group_sizes]\n",
    "            MI_matrix[i, j] = ee.mi(y_chosen, X_chosen, k = neighbors)\n",
    "            if isplot:\n",
    "                plot_matrices([MI_matrix], images_per_row = 4)\n",
    "    return MI_matrix, {}\n",
    "\n",
    "\n",
    "def get_conditional_MI(X, y, group_sizes = 1, neighbors = 3, assigned_target_id = None):\n",
    "    \"\"\"Obtain transfer entropy using conditional mutual information\"\"\"\n",
    "    N, Ny, K, K2, num_models, num_models_y, group_sizes_y, training_target_ids = \\\n",
    "        get_shapes(X, y, group_sizes, assigned_target_id)\n",
    "\n",
    "    conditional_MI = np.zeros((num_models_y, num_models))\n",
    "    for i in training_target_ids:\n",
    "        for j in range(num_models):\n",
    "            idx = torch.cat([torch.arange(j * group_sizes), torch.arange((j + 1) * group_sizes, N)]).long().to(device)\n",
    "            y_chosen = y[:, :, i * group_sizes_y: (i + 1) * group_sizes_y]\n",
    "            X_chosen = X[:, :, j * group_sizes: (j + 1) * group_sizes]\n",
    "            X_conditioned = X.index_select(2, idx)\n",
    "            conditional_MI[i, j] = ee.cmi(y_chosen, X_chosen, X_conditioned, k = neighbors)\n",
    "    return conditional_MI, {}\n",
    "\n",
    "\n",
    "def elastic_Net(\n",
    "    X,\n",
    "    y,\n",
    "    group_sizes = 1,\n",
    "    assigned_target_id = None,\n",
    "    l1_ratio = [0.5, 0.8, 0.9, 0.95, 0.99],\n",
    "    a = np.logspace(-4,-0.5,200),\n",
    "    nfold = 5,\n",
    "    tol = 1e-10,\n",
    "    ):\n",
    "    \"\"\"Elastic Net fitting with cross-validation\"\"\"\n",
    "    from sklearn import linear_model\n",
    "    from sklearn.model_selection import TimeSeriesSplit\n",
    "    N, Ny, K, K2, num_models, num_models_y, group_sizes_y, training_target_ids = \\\n",
    "        get_shapes(X, y, group_sizes, assigned_target_id)\n",
    "    coeff_matrix = np.zeros((num_models_y, num_models))\n",
    "\n",
    "    for i in training_target_ids:\n",
    "        fold = TimeSeriesSplit(n_splits=nfold, max_train_size=None)\n",
    "        model = linear_model.ElasticNetCV(l1_ratio, cv=fold, verbose=0, selection='random', tol=tol, alphas=a)\n",
    "        #        \n",
    "        model.fit(*to_np_array(*flatten(X, y[...,i * group_sizes : (i+1) * group_sizes])))\n",
    "        coeff = np.abs(model.coef_.reshape(X.shape[1:])).mean(0)\n",
    "        coeff_matrix[i] = coeff\n",
    "    return coeff_matrix, {}\n",
    "\n",
    "\n",
    "def get_Granger_linear_reg(X, y, group_sizes = 1, assigned_target_id = None, fit_intercept = False):\n",
    "    \"\"\"Obtain linear Granger causality using linear regression\"\"\"\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    N, Ny, K, K2, num_models, num_models_y, group_sizes_y, training_target_ids = \\\n",
    "        get_shapes(X, y, group_sizes, assigned_target_id)\n",
    "    X, y = to_np_array(X, y)\n",
    "\n",
    "    Granger_linear = np.zeros((num_models_y, num_models))\n",
    "    error_matrix = np.zeros(num_models_y)\n",
    "    error_matrix_ablated = np.zeros((num_models_y, num_models))\n",
    "    for i in training_target_ids:\n",
    "        model = LinearRegression(fit_intercept = fit_intercept)\n",
    "        y_chosen = flatten(y[:,:, i * group_sizes_y: (i+1) * group_sizes_y])\n",
    "        model.fit(flatten(X), y_chosen)\n",
    "        pred = model.predict(flatten(X))\n",
    "        error_matrix[i] = np.abs(pred - y_chosen).mean()\n",
    "\n",
    "        for j in range(num_models):\n",
    "            idx = np.concatenate([np.arange(j * group_sizes), np.arange((j + 1) * group_sizes, N)])\n",
    "            X_ablated = np.take(X, indices = idx, axis = 2)\n",
    "            model_minus_j = LinearRegression(fit_intercept = fit_intercept)\n",
    "            model_minus_j.fit(flatten(X_ablated), y_chosen)\n",
    "            pred = model_minus_j.predict(flatten(X_ablated))\n",
    "            error_matrix_ablated[i, j] = np.abs(pred - y_chosen).mean()\n",
    "            Granger_linear[i, j] = np.log(error_matrix_ablated[i, j] / error_matrix[i])\n",
    "    return Granger_linear, {}\n",
    "\n",
    "\n",
    "def get_model(input_size, struct_param, settings, is_uncertainty = False, loss_core = \"mse\", is_cuda = False):\n",
    "    \"\"\"Helper function for constructing the model\"\"\"\n",
    "    if is_uncertainty:\n",
    "        model_pred = MLP_noise(input_size = input_size,\n",
    "                         struct_param = struct_param,\n",
    "                         settings = settings,\n",
    "                         is_cuda = is_cuda,\n",
    "                        )\n",
    "        model_logstd = MLP_noise(input_size = input_size,\n",
    "                         struct_param = struct_param,\n",
    "                         settings = settings,\n",
    "                         is_cuda = is_cuda,\n",
    "                        )\n",
    "        model = Model_with_uncertainty(model_pred, model_logstd)\n",
    "        criterion = Loss_with_uncertainty(core = loss_core)\n",
    "        criterion_measure = get_criterion(loss_type = \"get_Variance\")\n",
    "    else:\n",
    "        model = MLP_noise(input_size = input_size,\n",
    "                    struct_param = struct_param,\n",
    "                    settings = settings,\n",
    "                    is_cuda = is_cuda,\n",
    "                   )\n",
    "        criterion = criterion_measure = get_criterion(loss_type = loss_core)\n",
    "    return model, criterion, criterion_measure\n",
    "\n",
    "\n",
    "def get_causal_influence(\n",
    "    X,\n",
    "    y,\n",
    "    validation_data,\n",
    "    group_sizes = 1,\n",
    "    average_times = 20,\n",
    "    is_uncertainty = False,\n",
    "    KL_estimator = \"ee\",\n",
    "    causality_truth = None,\n",
    "    isplot = True,\n",
    "    verbose = True,\n",
    "    noise_amp = None,\n",
    "    permute_mode = \"permute\",\n",
    "    **kwargs\n",
    "    ):\n",
    "    \"\"\"Get causal_influence based on NN fitting\n",
    "    The causal influence formula is given by Janzing, Dominik, et al. \"Quantifying causal influences\", \n",
    "    The Annals of Statistics 41.5 (2013): 2324-2358. The model is firstly learned from data.\n",
    "    \"\"\"\n",
    "#     X = X_train\n",
    "#     y = y_train\n",
    "#     validation_data = (X_test, y_test)\n",
    "#     KL_estimator = \"ee\"\n",
    "#     kwargs = {}\n",
    "#     causality_truth = None\n",
    "#     is_uncertainty = False\n",
    "#     isplot = True\n",
    "#     average_times = 3\n",
    "#     verbose = True\n",
    "\n",
    "    X_valid, y_valid = validation_data\n",
    "    struct_param = kwargs[\"struct_param\"] if \"struct_param\" in kwargs else [[(K2, group_sizes), \"Simple_Layer\", {\"activation\": \"linear\"}]]\n",
    "    settings = kwargs[\"settings\"] if \"settings\" in kwargs else {}\n",
    "    if \"epochs\" not in kwargs:\n",
    "        kwargs[\"epochs\"] = 15000   \n",
    "    N, Ny, K, K2, num_models, num_models_y, group_sizes_y, training_target_ids = \\\n",
    "            get_shapes(X, y, group_sizes, assigned_target_id)\n",
    "    \n",
    "    input_size = (K, N)\n",
    "\n",
    "    info_all = {}\n",
    "    info_all[\"mse_full\"] = - np.ones(num_models_y)\n",
    "    info_all[\"mse_permuted\"] = - np.ones((num_models_y, num_models, average_times))\n",
    "    info_all[\"log_mse_ratio\"] = - np.ones((num_models_y, num_models, average_times))\n",
    "    info_all[\"causal_influence_all\"] = np.zeros((num_models_y, num_models, average_times))\n",
    "    info_all[\"causal_influence\"] = np.zeros((num_models_y, num_models)) \n",
    "\n",
    "    if causality_truth is not None:\n",
    "        if len(causality_truth.shape) == 3:\n",
    "            causality_truth = causality_truth.any(-2, keepdims = True).squeeze()\n",
    "\n",
    "    for i in training_target_ids:\n",
    "        model, criterion, criterion_measure = get_model(input_size = input_size,\n",
    "                                                        struct_param = struct_param,\n",
    "                                                        settings = settings,\n",
    "                                                        is_uncertainty = is_uncertainty,\n",
    "                                                        loss_core = loss_core,\n",
    "                                                        is_cuda = is_cuda,\n",
    "                                                       )\n",
    "        criterion_measure_full = deepcopy(criterion_measure)\n",
    "        criterion_measure_full.reduce = False\n",
    "        if noise_amp is None:\n",
    "            _ = train(model, X, y[:,:, i*group_sizes_y: (i+1)*group_sizes_y], criterion = criterion, isplot = isplot, **kwargs)  \n",
    "        else:\n",
    "            _ = train(model, X, y[:,:, i*group_sizes_y: (i+1)*group_sizes_y], criterion = criterion, isplot = isplot, noise_amp = noise_amp, **kwargs)\n",
    "        info_all[\"model_full_{0}\".format(i)] = model.model_dict\n",
    "        info_all[\"mse_full\"][i] = to_np_array(model.get_loss(X_valid, y_valid[:,:, i*group_sizes_y: (i+1)*group_sizes_y], criterion_measure))\n",
    "        if verbose:\n",
    "            print(\"fitting full model {0},       mse = {1:.6f}\".format(i, info_all[\"mse_full\"][i]))\n",
    "            try:\n",
    "                sys.stdout.flush()\n",
    "            except:\n",
    "                pass\n",
    "        for j in range(num_models):\n",
    "            for k in range(average_times):\n",
    "                print(\"j={0}\\t k={1}\".format(j,k))\n",
    "                X_permuted = permute_dim(X, 2, j, group_sizes, mode = permute_mode)\n",
    "                y_prime = model(X_permuted)\n",
    "                concat_prev = torch.cat([X.view(X.size(0), -1), y[:,:, i*group_sizes_y: (i+1)*group_sizes_y].view(y.size(0), -1)], 1)\n",
    "                concat_prime = torch.cat([X_permuted.view(X_permuted.size(0), -1), y_prime.view(y_prime.size(0), -1)], 1)\n",
    "                if KL_estimator == \"ee\":\n",
    "                    info_all[\"causal_influence_all\"][i, j, k] = ee.kldiv(to_np_array(concat_prev), to_np_array(concat_prime))\n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "            info_all[\"causal_influence\"][i, j] = info_all[\"causal_influence_all\"][i,j].mean()\n",
    "        if isplot == 2 or (isplot == 1 and j == num_models - 1):\n",
    "            if causality_truth is not None:\n",
    "                plot_matrices([info_all[\"causal_influence\"], causality_truth], subtitles = [\"causal_influence\", \"Truth\"], images_per_row = 5)\n",
    "            else:\n",
    "                plot_matrices([info_all[\"causal_influence\"]], subtitles = [\"causal_influence\"], images_per_row = 4)\n",
    "        if A_whole is not None:\n",
    "            try:\n",
    "                get_AUCs(info_all[\"causal_influence\"][:i+1], A_whole[:i+1], neglect_idx = neglect_idx)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    return info_all[\"causal_influence\"], info_all\n",
    "\n",
    "\n",
    "\n",
    "class MPIR(object):\n",
    "    \"\"\"Our Causal Learning with Minimum Predictive information regularization (MPIR) method\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        ):\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    def train(\n",
    "        self,\n",
    "        X,\n",
    "        y,\n",
    "        validation_data,\n",
    "        group_sizes = 1,\n",
    "        noise_type = \"uniform-series\",\n",
    "        noise_loss_scale = 1e-2,\n",
    "        norm_mode = \"info\",\n",
    "        info_estimate_mode = \"diag\",\n",
    "        is_uncertainty = False,\n",
    "        A_whole = None,\n",
    "        assigned_target_id = None,\n",
    "        isplot = True,\n",
    "        verbose = True,\n",
    "        **kwargs\n",
    "        ):\n",
    "        # Obtain settings:\n",
    "        device = torch.device(\"cuda\" if is_cuda else \"cpu\")\n",
    "        struct_param = kwargs[\"struct_param\"] if \"struct_param\" in kwargs else [[10, \"Simple_Layer\", {\"activation\": \"leakyRelu\"}], [(K2, group_sizes), \"Simple_Layer\", {\"activation\": \"linear\"}]]\n",
    "        settings = kwargs[\"settings\"] if \"settings\" in kwargs else {}\n",
    "\n",
    "        model_type = kwargs[\"model_type\"] if \"model_type\" in kwargs else \"MLP\"\n",
    "        loss_core = kwargs[\"loss_core\"] if \"loss_core\" in kwargs else \"mse\"\n",
    "        added_noise_type = kwargs[\"added_noise_type\"] if \"added_noise_type\" in kwargs else \"Gaussian\"\n",
    "        lr = kwargs[\"lr\"] if \"lr\" in kwargs else 1e-4\n",
    "        reg_amp = kwargs[\"reg_amp\"] if \"reg_amp\" in kwargs else 1e-4\n",
    "        batch_size = kwargs[\"batch_size\"] if \"batch_size\" in kwargs else 1000\n",
    "        warmup = kwargs[\"warmup\"] if \"warmup\" in kwargs else 400\n",
    "\n",
    "        permute_mode = kwargs[\"permute_mode\"] if \"permute_mode\" in kwargs else \"permute\"\n",
    "        is_plot_MI = kwargs[\"is_plot_MI\"] if \"is_plot_MI\" in kwargs else True\n",
    "        num_samples = kwargs[\"num_samples\"] if \"num_samples\" in kwargs else 1\n",
    "        max_iter = kwargs[\"max_iter\"] if \"max_iter\" in kwargs else 10000\n",
    "        inspect_interval = kwargs[\"inspect_interval\"] if \"inspect_interval\" in kwargs else 20\n",
    "        plot_interval = kwargs[\"plot_interval\"] if \"plot_interval\" in kwargs else 200\n",
    "        save_interval = kwargs[\"save_interval\"] if \"save_interval\" in kwargs else 1000\n",
    "        patience = kwargs[\"patience\"] if \"patience\" in kwargs else 40\n",
    "        is_log = kwargs[\"is_log\"] if \"is_log\" in kwargs else False\n",
    "        record_mode = kwargs[\"record_mode\"] if \"record_mode\" in kwargs else 0\n",
    "        num_plots = 3\n",
    "\n",
    "        # Obtain various shapes:\n",
    "        N, Ny, K, K2, num_models, num_models_y, group_sizes_y, training_target_ids = \\\n",
    "            get_shapes(X, y, group_sizes, assigned_target_id)\n",
    "        X_test, y_test = validation_data\n",
    "        input_size = (K, N)\n",
    "        self.info_dict = {}\n",
    "\n",
    "        def get_X_tilde(X, X_std, noise_amp):\n",
    "            noise_amp_core = X_std * expand_tensor(noise_amp, -1, group_sizes)\n",
    "            return X + torch.randn(X.size()).to(device) * noise_amp_core\n",
    "\n",
    "        self.data_record = {i: {} for i in range(num_models)}\n",
    "        logger = Logger('./logs')\n",
    "\n",
    "        if isplot:\n",
    "            if \"A_whole\" in locals() and A_whole is not None:\n",
    "                print(\"|A|:\")\n",
    "                if len(A_whole.squeeze().shape) > 2:\n",
    "                    plot_matrices(np.abs(A_whole)[:num_plots], images_per_row = 3)\n",
    "                else:\n",
    "                    plot_matrices([np.abs(A_whole.squeeze())], images_per_row = 3)\n",
    "\n",
    "                    \n",
    "        # Initialize metrics:\n",
    "        if noise_type == \"uniform-series\":\n",
    "            self.noise_amp_all = torch.zeros(num_models_y, num_models)\n",
    "        elif noise_type == \"fully-random\":\n",
    "            self.noise_amp_all = torch.zeros(num_models_y, K, N)\n",
    "        else:\n",
    "            raise\n",
    "        self.info_norm_all = torch.zeros(num_models_y, num_models)\n",
    "        self.negative_log_noise_amp_all = torch.zeros(num_models_y, num_models)\n",
    "        self.causality_pred_all = torch.zeros(num_models_y, num_models)\n",
    "\n",
    "        X_std = X.std(0)\n",
    "        # Main training loop:\n",
    "        for target_id in training_target_ids:\n",
    "            self.target_id = target_id\n",
    "            print(\"Perform id: {0}\".format(target_id))\n",
    "            self.model, criterion, _ = get_model(input_size = input_size,\n",
    "                                                struct_param = struct_param,\n",
    "                                                settings = settings,\n",
    "                                                is_uncertainty = False,\n",
    "                                                loss_core = loss_core,\n",
    "                                                is_cuda = is_cuda,\n",
    "                                               )\n",
    "\n",
    "            # Initialize noise_amp variable:\n",
    "            if noise_type == \"fully-random\":\n",
    "                noise_amp = torch.tensor((np.ones((K, N)) * 0.01).tolist(), requires_grad = True, device = device)\n",
    "            elif noise_type == \"uniform-series\":\n",
    "                noise_amp = torch.tensor((np.ones((1, int(N / group_sizes))) * 0.01).tolist(), requires_grad = True, device = device)\n",
    "            else:\n",
    "                raise Exception(\"noise_type {0} not recognized!\".format(noise_type))    \n",
    "\n",
    "            # If using variational upper bound for mutual information, first initialize the mixture Gaussians appropriately:\n",
    "            if info_estimate_mode == \"var\":\n",
    "                self.variational_entropy = Variational_Entropy(num_models = N // group_sizes,\n",
    "                                                               num_components = 10,\n",
    "                                                               dim = K * group_sizes,\n",
    "                                                              )\n",
    "                self.variational_entropy.initialize(partition(X, group_sizes), num_samples = 5)\n",
    "\n",
    "            param_to_optimize = [{\"params\": self.model.parameters()}]\n",
    "            param_to_optimize.append({\"params\": noise_amp})\n",
    "            self.optimizer = optim.Adam(param_to_optimize, lr = lr)\n",
    "            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor = 0.2, patience = 10)\n",
    "            self.early_stopping = Early_Stopping(patience = patience)\n",
    "\n",
    "            dataset_train = data_utils.TensorDataset(X, y[:, :, target_id * group_sizes_y: (target_id + 1) * group_sizes_y])\n",
    "            train_loader = data_utils.DataLoader(dataset_train, batch_size = batch_size, shuffle = True)\n",
    "            to_stop = False\n",
    "\n",
    "            # Train the model:\n",
    "            for i in range(max_iter):\n",
    "                if info_estimate_mode == \"var\" and i % 10 == 0 and i >= warmup:\n",
    "                    print(\"training variational entropy estimator:\")\n",
    "                    for batch_id, (X_batch, y_batch) in enumerate(train_loader):\n",
    "                        X_batch_new = get_X_tilde(X_batch, X_std, noise_amp)\n",
    "                        self.variational_entropy.train(partition(X_batch_new, group_sizes), num_steps = 1)\n",
    "\n",
    "                for batch_id, (X_batch, y_batch) in enumerate(train_loader):\n",
    "                    self.optimizer.zero_grad()\n",
    "                    reg = self.model.get_regularization(source = [\"weight\", \"bias\"], mode = \"L1\") * reg_amp\n",
    "                    loss_train_total = torch.Tensor([0]).to(device)\n",
    "                    for j in range(num_samples):\n",
    "                        if noise_type == \"uniform-series\":\n",
    "                            noise_amp_core = X_std * expand_tensor(noise_amp, -1, group_sizes)\n",
    "                        else:\n",
    "                            noise_amp_core = X_std * noise_amp\n",
    "                        loss_train = self.model.get_loss(X_batch, y_batch, criterion, noise_amp = noise_amp_core, added_noise_type = added_noise_type)\n",
    "                        loss_train_total = loss_train_total + loss_train\n",
    "                    loss_train_total = loss_train_total / num_samples\n",
    "\n",
    "                    loss_total = loss_train_total + reg * reg_amp\n",
    "                    if i > warmup:\n",
    "                        if noise_type == \"uniform-series\":\n",
    "                            noise_amp_core = X_std * expand_tensor(noise_amp, -1, group_sizes)\n",
    "                        else:\n",
    "                            noise_amp_core = X_std * noise_amp\n",
    "\n",
    "                        if info_estimate_mode == \"diag\":\n",
    "                            if noise_type == \"uniform-series\":\n",
    "                                info_estimate = norm(noise_amp, noise_mode = noise_mode, mode = norm_mode) * K * group_sizes\n",
    "                            else:\n",
    "                                info_estimate = norm(noise_amp, noise_mode = noise_mode, mode = norm_mode)\n",
    "                        elif info_estimate_mode == \"Gauss\":\n",
    "                            info_estimate = (get_entropy_Gaussian_list(partition(X_batch, group_sizes)) - get_noise_entropy(noise_amp_core, K, group_sizes)).sum()\n",
    "                        elif info_estimate_mode == \"var\":\n",
    "                            info_estimate = (self.variational_entropy.get_entropy(X_batch, noise_amp_core, group_sizes) - get_noise_entropy(noise_amp_core, K, group_sizes)).sum()\n",
    "                        else:\n",
    "                            raise\n",
    "                        loss_total = loss_total + noise_loss_scale * info_estimate\n",
    "                    loss_total.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                # Validation and output:\n",
    "                loss_test = self.model.get_loss(X_test, y_test[:, :, target_id * group_sizes_y : (target_id + 1) * group_sizes_y], criterion)\n",
    "                if record_mode > 0:\n",
    "                    record_data(self.data_record[target_id], [loss_train.item(), loss_test.item(), reg.item(), i], [\"loss_train\", \"loss_test\", \"reg\", \"iter\"])\n",
    "                if i % inspect_interval == 0 or to_stop:\n",
    "                    self.scheduler.step(loss_test.item())\n",
    "                    to_stop = self.early_stopping.monitor(loss_test.item())\n",
    "                    self.noise_amp_all[target_id] = noise_amp.abs()\n",
    "                    print(\"iter {0}  \\tloss_total: {1:.6f}\\tloss_train: {2:.6f}\\tloss_test: {3:.6f}\\tnoise_norm: {4:.6f}\\treg: {5:.9f}\\tlr = {6:.6f}\".format(\n",
    "                        i, loss_total.item(), (loss_train_total / num_samples).item(), loss_test.item(), noise_loss_scale * norm(self.noise_amp_all[target_id], noise_mode = noise_mode, mode = norm_mode).item(), reg.item() * reg_amp, self.optimizer.param_groups[0][\"lr\"]), end = \"\")\n",
    "                    try:\n",
    "                        sys.stdout.flush()\n",
    "                    except:\n",
    "                        pass\n",
    "                    print()\n",
    "                    if is_log:\n",
    "                        # Tensorboard Logging:\n",
    "                        # 1. Log scalar values (scalar summary)\n",
    "                        info = {'loss_train': loss_train.item(), 'loss_test': loss_test.item()}\n",
    "                        for tag, value in info.items():\n",
    "                            logger.log_scalar(tag, value, i+1)\n",
    "\n",
    "                        # 2. Log values and gradients of the parameters (histogram summary)\n",
    "                        for tag, value in self.model.named_parameters():\n",
    "                            tag = tag.replace('.', '/')\n",
    "                            logger.log_histogram(tag, value.data.cpu().numpy(), i+1)\n",
    "                            logger.log_histogram(tag + '/grad', value.grad.data.cpu().numpy(), i+1)\n",
    "\n",
    "                if i % plot_interval == 0:\n",
    "                    if isplot:\n",
    "                        plt.figure(figsize = (8,6))\n",
    "                        plt.plot(self.data_record[target_id][\"loss_train\"], label = \"loss_train\")\n",
    "                        plt.plot(self.data_record[target_id][\"loss_test\"], label = \"loss_test\")\n",
    "                        plt.legend()\n",
    "                        plt.show()\n",
    "                        plt.figure(figsize = (8,6))\n",
    "                        plt.semilogy(self.data_record[target_id][\"loss_train\"], label = \"loss_train\")\n",
    "                        plt.semilogy(self.data_record[target_id][\"loss_test\"], label = \"loss_test\")\n",
    "                        plt.legend()\n",
    "                        plt.show()\n",
    "\n",
    "                    if noise_type == \"uniform-series\":\n",
    "                        self.info_norm_all[target_id] = torch.log2(1 + 1 / self.noise_amp_all[target_id] ** 2) / 2\n",
    "                        self.negative_log_noise_amp_all[target_id] = -torch.log2(self.noise_amp_all[target_id])\n",
    "                    else:\n",
    "                        self.info_norm_all[target_id] = shrink_tensor((torch.log2(1 + 1 / self.noise_amp_all[target_id] ** 2) / 2).sum(-2), -1, group_sizes, \"sum\")\n",
    "                        self.negative_log_noise_amp_all[target_id] = shrink_tensor(-torch.log2(self.noise_amp_all[target_id]).sum(-2), -1, group_sizes, \"sum\")\n",
    "                    threshold = plot_clusters(self.info_norm_all[target_id: target_id + 1], isplot = isplot)\n",
    "                    self.causality_pred_all[target_id] = (self.info_norm_all[target_id] > threshold).byte()\n",
    "\n",
    "                    if A_whole is not None:\n",
    "                        self.causality_truth_cal = np.abs(A_whole) > 0\n",
    "                        self.causality_truth_cal = self.causality_truth_cal.any(-2, keepdims = True)\n",
    "\n",
    "                        #Precision and recall:\n",
    "                        try:\n",
    "                            precision, recall, F1, _ = prfs(self.causality_truth_cal[:target_id + 1].flatten(), to_np_array(self.causality_pred_all[:target_id + 1]).flatten())\n",
    "                            ROC_AUC = roc_auc_score(self.causality_truth_cal[:target_id + 1].flatten(), to_np_array(self.negative_log_noise_amp_all[:target_id + 1].view(-1)))\n",
    "                            fpr, tpr, _ = roc_curve(self.causality_truth_cal[:target_id + 1].flatten(), to_np_array(self.negative_log_noise_amp_all[:target_id + 1].view(-1)))\n",
    "                            precision_curve, recall_curve, _ = precision_recall_curve(self.causality_truth_cal[:target_id + 1].flatten(), to_np_array(self.negative_log_noise_amp_all[:target_id + 1].view(-1)))\n",
    "                            PR_AUC = auc(recall_curve, precision_curve)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        if isplot:\n",
    "                            plt.figure(figsize = (8, 6))\n",
    "                            plt.plot(fpr, tpr)\n",
    "                            plt.plot([0,1], [0,1], \"k--\")\n",
    "                            plt.xlabel(\"False Positive Rate\")\n",
    "                            plt.ylabel(\"True Positive Rate\")\n",
    "                            plt.xlim([0,1.05])\n",
    "                            plt.ylim([0,1.05])\n",
    "                            plt.title(\"ROC curve\")\n",
    "                            plt.show()\n",
    "\n",
    "                            plt.figure(figsize = (8, 6))\n",
    "                            plt.plot(precision_curve, recall_curve)\n",
    "                            plt.xlabel(\"Precision\")\n",
    "                            plt.ylabel(\"Recall\")\n",
    "                            plt.xlim([0,1.05])\n",
    "                            plt.ylim([0,1.05])\n",
    "                            plt.title(\"Precision-Recall curve\")\n",
    "                            plt.show()\n",
    "\n",
    "                            if i > 0:\n",
    "                                plt.figure(figsize = (8, 6))\n",
    "                                plt.plot(self.data_record[target_id][\"iter_interval\"], self.data_record[target_id][\"precision\"], label = \"Precision\")\n",
    "                                plt.plot(self.data_record[target_id][\"iter_interval\"], self.data_record[target_id][\"recall\"], label = \"Recall\")\n",
    "                                plt.plot(self.data_record[target_id][\"iter_interval\"], self.data_record[target_id][\"F1\"], label = \"F1\")\n",
    "                                plt.plot(self.data_record[target_id][\"iter_interval\"], self.data_record[target_id][\"ROC_AUC\"], label = \"ROC-AUC\")\n",
    "                                plt.plot(self.data_record[target_id][\"iter_interval\"], self.data_record[target_id][\"PR_AUC\"], label = \"PR-AUC\")\n",
    "                                plt.legend()\n",
    "                                plt.show()\n",
    "\n",
    "                    if is_plot_MI:\n",
    "                        MI_xn_x = get_MIs(X, y, noise_amp_all = self.noise_amp_all[:target_id + 1], group_sizes = group_sizes, mode = \"xn-x\", noise_type = noise_type)\n",
    "                        MI_xn_y = get_MIs(X, y, noise_amp_all = self.noise_amp_all[:target_id + 1], group_sizes = group_sizes, mode = \"xn-y\", noise_type = noise_type)\n",
    "                        MI_x_y = get_MIs(X, y, noise_amp_all = self.noise_amp_all[:target_id + 1], group_sizes = group_sizes, mode = \"x-y\", noise_type = noise_type)\n",
    "                        if record_mode > 0:\n",
    "                            record_data(self.data_record[target_id], [MI_xn_x, MI_xn_y, MI_x_y], [\"MI_xn-x\", \"MI_xn-y\", \"MI_x-y\"])\n",
    "\n",
    "                    if isplot:\n",
    "                        if num_models > 1 and noise_type == \"fully-random\":\n",
    "                            plot_matrices(self.noise_amp_all, images_per_row = 5)\n",
    "                        if A_whole is not None:\n",
    "                            plot_matrices([to_np_array(self.info_norm_all), to_np_array(self.negative_log_noise_amp_all.squeeze()), to_np_array(self.causality_pred_all.squeeze()), self.causality_truth_cal.squeeze()], images_per_row = 5,\n",
    "                                          subtitles = [\"info-norm\", \"-log10(Eta)\", \"causality prediction\", \"true matrix\"],\n",
    "                                         )\n",
    "                            if is_plot_MI:\n",
    "                                plot_matrices([MI_xn_x, MI_xn_y, MI_x_y], images_per_row = 5, subtitles = [\"MI_xn-x\", \"MI_xn-y\", \"MI_x-y\"])\n",
    "                        else:\n",
    "                            plot_matrices([to_np_array(self.info_norm_all), to_np_array(self.negative_log_noise_amp_all.squeeze()), to_np_array(self.causality_pred_all.squeeze())], images_per_row = 4,\n",
    "                                          subtitles = [\"info-norm\", \"-log10(Eta)\", \"causality prediction\"],\n",
    "                                         )\n",
    "                            if is_plot_MI:\n",
    "                                plot_matrices([MI_xn_x, MI_xn_y, MI_x_y], images_per_row = 4, subtitles = [\"MI_xn-x\", \"MI_xn-y\", \"MI_x_y\"])\n",
    "                    if record_mode > 0:\n",
    "                        record_data(self.data_record[target_id], [to_np_array(self.noise_amp_all), self.info_norm_all, i], [\"noise_amp\", \"info_norm_all\", \"iter_interval\"])\n",
    "\n",
    "\n",
    "                    if A_whole is not None:\n",
    "                        try:\n",
    "                            if record_mode > 0:\n",
    "                                record_data(self.data_record[target_id], [precision[1], recall[1], F1[1], ROC_AUC, PR_AUC], [\"precision\", \"recall\", \"F1\", \"ROC_AUC\", \"PR_AUC\"])\n",
    "                            print(\"Causality prediction:\\nprecision: {0:.9f}\\trecall: {1:.9f}\\tF1 : {2:.9f}\\tROC_AUC: {3:.9f}\\tPR_AUC: {4:.9f}\".format(precision[1], recall[1], F1[1], ROC_AUC, PR_AUC))\n",
    "                        except:\n",
    "                            pass\n",
    "                    print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "\n",
    "                # Saving:\n",
    "                if i % save_interval == 0:\n",
    "                    self.info_dict[\"data_record\"] = self.data_record\n",
    "                    self.info_dict[\"noise_amp_all\"] = self.noise_amp_all\n",
    "                    self.info_dict[\"info_norm_all\"] = self.info_norm_all\n",
    "                    self.info_dict[\"causality_pred_all\"] = self.causality_pred_all\n",
    "                    self.info_dict[\"negative_log_noise_amp_all\"] = self.negative_log_noise_amp_all\n",
    "                    if is_plot_MI:\n",
    "                        self.info_dict[\"MI_xn_x\"] = MI_xn_x\n",
    "                        self.info_dict[\"MI_xn_y\"] = MI_xn_y\n",
    "                        self.info_dict[\"MI_x_y\"] = MI_x_y\n",
    "\n",
    "                if to_stop:\n",
    "                    print(\"Early stopping at iter {0}\".format(i))\n",
    "                    break\n",
    "            self.info_dict[\"model_full_{0}\".format(target_id)] = self.model.model_dict\n",
    "        return to_np_array(self.info_norm_all), self.info_dict\n",
    "\n",
    "\n",
    "def plot_comparison(matrix, A_whole, title, isplot = True):\n",
    "    if isplot:\n",
    "        if A_whole is not None:\n",
    "            causality_truth = np.abs(A_whole) > 0\n",
    "            if noise_type == \"uniform-series\":\n",
    "                causality_truth = causality_truth.any(-2, keepdims = True)\n",
    "            plot_matrices([matrix, causality_truth.squeeze()], subtitles = [title, \"Truth\"], images_per_row = 4)\n",
    "        else:\n",
    "            plot_matrices([matrix], subtitles = [title])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal learning with chosen method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The output is saved in the item_dict[\"result\"], where item_dict[\"result\"][0] is the causal matrix, where the (i, j)\n",
    "    element is the inferred causal strength from j to i.\n",
    "The item_dict[\"metrics\"] records the AUC-PR and AUC-ROC metrics, compared with the true causal matrix A_whole. A_whole has \n",
    "    dimension of (num_time_series, K, num_time_series), where num_time_series = N // group_sizes, with its (i, k, j) element\n",
    "    indicating the true causal strength from j to i at a lag of k.\n",
    "\"\"\"\n",
    "\n",
    "learned_dict = {}\n",
    "item_dict = {}\n",
    "Ny_size = y.shape[-1] if X.shape[-1] != y.shape[-1] else group_sizes\n",
    "struct_param = [[num_neurons, \"Simple_Layer\", {}] for num_neurons in struct_tuple] + [[(K2, Ny_size), \"Simple_Layer\", {\"activation\": \"linear\"}]]\n",
    "settings = {\"activation\": activation}\n",
    "if method[0] == \"MI\":\n",
    "    \"\"\"Mutual information method\"\"\"\n",
    "    learned_dict[method[0]] = item_dict\n",
    "    neighbors = method[1]\n",
    "    item_dict[\"result\"] = get_mutual_information(X_train, y_train, \n",
    "                                                 group_sizes = group_sizes, \n",
    "                                                 neighbors = neighbors, \n",
    "                                                 assigned_target_id = assigned_target_id, \n",
    "                                                 isplot = isplot,\n",
    "                                                )\n",
    "    plot_comparison(item_dict[\"result\"][0], A_whole, \"mutual_information\", isplot = isplot)\n",
    "    item_dict[\"metrics\"] = get_AUCs(item_dict[\"result\"][0], A_whole, neglect_idx = neglect_idx)\n",
    "    pickle.dump(learned_dict, open(filename, \"wb\"))\n",
    "\n",
    "\n",
    "elif method[0] == \"trans-entropy\":\n",
    "    \"\"\"Transfer entropy method\"\"\"\n",
    "    learned_dict[method[0]] = item_dict\n",
    "    neighbors = method[1]\n",
    "    item_dict[\"result\"] = get_conditional_MI(X_train, y_train, group_sizes = group_sizes, neighbors = neighbors, assigned_target_id = assigned_target_id)\n",
    "    plot_comparison(item_dict[\"result\"][0], A_whole, \"conditional_MI\", isplot = isplot)\n",
    "    item_dict[\"metrics\"] = get_AUCs(item_dict[\"result\"][0], A_whole, neglect_idx = neglect_idx)\n",
    "    pickle.dump(learned_dict, open(filename, \"wb\"))\n",
    "\n",
    "\n",
    "elif method[0] == \"G-linear\":\n",
    "    \"\"\"Linear Granger method\"\"\"\n",
    "    fit_intercept = method[1]\n",
    "    learned_dict[method[0]] = item_dict\n",
    "    item_dict[\"result\"] = get_Granger_linear_reg(X_train, y_train, group_sizes = group_sizes, assigned_target_id = assigned_target_id, fit_intercept = fit_intercept)\n",
    "    plot_comparison(item_dict[\"result\"][0], A_whole, \"Granger_causality\", isplot = isplot)\n",
    "    try:\n",
    "        item_dict[\"metrics\"] = get_AUCs(item_dict[\"result\"][0], A_whole, neglect_idx = neglect_idx)\n",
    "    except:\n",
    "        item_dict[\"metrics\"] = None\n",
    "    pickle.dump(learned_dict, open(filename, \"wb\"))\n",
    "    \n",
    "\n",
    "elif method[0] == \"elasticNet\":\n",
    "    \"\"\"Elastic Net method\"\"\"\n",
    "    learned_dict[method[0]] = item_dict\n",
    "    item_dict[\"result\"] = elastic_Net(X_train, y_train,\n",
    "                                      group_sizes = group_sizes,\n",
    "                                      assigned_target_id = assigned_target_id,\n",
    "                                     )\n",
    "    plot_comparison(item_dict[\"result\"][0], A_whole, \"elasticNet\", isplot = isplot)\n",
    "    item_dict[\"metrics\"] = get_AUCs(item_dict[\"result\"][0], A_whole, neglect_idx = neglect_idx)\n",
    "    pickle.dump(learned_dict, open(filename, \"wb\"))\n",
    "\n",
    "\n",
    "elif method[0] == \"causal-influence\":\n",
    "    \"\"\"Causal Influence method:\"\"\"\n",
    "    learned_dict[method[0]] = item_dict\n",
    "    reg_dict = {\"weight\": 1e-7, \"bias\": 1e-7}\n",
    "    average_times = 5\n",
    "    noise_amp = method[1]\n",
    "    if noise_amp is not None:\n",
    "        noise_amp = torch.ones(N).to(device) * noise_amp\n",
    "        X_std = X_train.view(-1, N).std(0)\n",
    "        noise_amp = noise_amp * X_std\n",
    "    item_dict[\"result\"] = get_causal_influence(X_train, y_train,\n",
    "                                              validation_data = (X_test, y_test),\n",
    "                                              group_sizes = group_sizes,\n",
    "                                              struct_param = struct_param,\n",
    "                                              average_times = average_times,\n",
    "                                              causality_truth = causality_truth,\n",
    "                                              noise_amp = noise_amp,\n",
    "                                              settings = settings,\n",
    "                                              is_cuda = is_cuda,\n",
    "                                              patience = None,\n",
    "                                              reg_dict = reg_dict,\n",
    "                                              isplot = isplot,\n",
    "                                             )\n",
    "    item_dict[\"metrics\"] = get_AUCs(item_dict[\"result\"][0], A_whole, neglect_idx = neglect_idx)\n",
    "    pickle.dump(learned_dict, open(filename, \"wb\"))\n",
    "\n",
    "elif method[0] == \"MPIR\":\n",
    "    \"\"\"Our MPIR method\"\"\"\n",
    "    noise_type = method[1]\n",
    "    added_noise_type = method[2]\n",
    "    noise_loss_scale = method[3]\n",
    "    norm_mode = method[4]\n",
    "    info_estimate_mode = method[5]\n",
    "    warmup = 400\n",
    "    max_iter = 30000\n",
    "    learned_dict[method[0]] = item_dict\n",
    "    record_mode = 1 if isplot else 0\n",
    "    patience = None\n",
    "    mpir = MPIR()\n",
    "    item_dict[\"result\"] = mpir.train(\n",
    "        X = X_train,\n",
    "        y = y_train,\n",
    "        validation_data = (X_test, y_test),\n",
    "        group_sizes = group_sizes,\n",
    "        noise_type = noise_type,\n",
    "        noise_loss_scale = noise_loss_scale,\n",
    "        norm_mode = norm_mode,\n",
    "        info_estimate_mode = info_estimate_mode,\n",
    "        added_noise_type = added_noise_type,\n",
    "        struct_param = struct_param,\n",
    "        settings = settings,\n",
    "        A_whole = A_whole,\n",
    "        assigned_target_id = assigned_target_id,\n",
    "        loss_core = loss_core,\n",
    "        lr = lr,\n",
    "        reg_amp = reg_amp,\n",
    "        batch_size = batch_size,\n",
    "        warmup = warmup,\n",
    "        max_iter = max_iter,\n",
    "        patience = patience,\n",
    "        record_mode = record_mode,\n",
    "        isplot = isplot,\n",
    "        is_plot_MI = False,\n",
    "        inspect_interval = 1 if info_estimate_mode == \"var\" else 20,\n",
    "        plot_interval = 5 if info_estimate_mode == \"var\" else 200\n",
    "    )\n",
    "    item_dict[\"metrics\"] = get_AUCs(item_dict[\"result\"][0], A_whole, neglect_idx = neglect_idx)\n",
    "    pickle.dump(learned_dict, open(filename, \"wb\"))\n",
    "\n",
    "else:\n",
    "    raise Exception(\"Method {0} not valid!\".format(method[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
